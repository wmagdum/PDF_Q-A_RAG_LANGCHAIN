{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3c97cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── standard library & env ─────────────────────────────────────────────────────\n",
    "import os                            # read API keys / file paths\n",
    "from dotenv import load_dotenv       # load OPENAI_API_KEY (and others) from .env\n",
    "\n",
    "# ── document loaders (ingest your files) ───────────────────────────────────────\n",
    "# Use only what you need; these come from langchain_community\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,                     # load PDFs page-wise\n",
    "    DirectoryLoader,                 # load a whole folder of files\n",
    "    Docx2txtLoader,                  # .docx support (optional)\n",
    "    CSVLoader,                       # .csv support (optional)\n",
    "    UnstructuredMarkdownLoader,      # .md support (optional; requires unstructured)\n",
    ")\n",
    "\n",
    "# ── core data types ────────────────────────────────────────────────────────────\n",
    "from langchain_core.documents import Document  # LangChain's Document wrapper\n",
    "\n",
    "# ── text splitting / chunking ─────────────────────────────────────────────────\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# smart splitter that respects boundaries and target chunk sizes\n",
    "\n",
    "# ── embeddings & LLMs (OpenAI stack; swap if you prefer HF, etc.) ─────────────\n",
    "from langchain_openai import OpenAIEmbeddings   # create dense vectors for chunks\n",
    "from langchain_openai import ChatOpenAI         # chat LLM used to answer with context\n",
    "\n",
    "# ── vector store (where chunks+embeddings live) ────────────────────────────────\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# in-memory/on-disk ANN index; easy to start with, fast on a single machine\n",
    "\n",
    "# ── prompts & chains (wire retriever + LLM together) ───────────────────────────\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# builds a “stuff” chain that injects retrieved docs into a prompt\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "# connects your retriever (FAISS) with the document-combining LLM chain\n",
    "\n",
    "# ── (optional) runnable & parsing utilities for custom pipelines ──────────────\n",
    "from langchain_core.runnables import RunnablePassthrough  # for custom LCEL graphs\n",
    "from langchain_core.output_parsers import StrOutputParser # parse LLM output to text\n",
    "\n",
    "# ── (optional) typing & simple logging ─────────────────────────────────────────\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52287269",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"D:\\LANGCHAIN\\Projects\\PDF_QA\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2d48c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_documents(path: str):\n",
    "    \"\"\"\n",
    "    Simple ingestion: loads a PDF from given path and returns list of Documents.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(path)   # only handles PDF\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a0d7d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 0, 'page_label': '30597'}, page_content='Received 1 February 2025, accepted 11 February 2025, date of publication 14 February 2025, date of current version 20 February 2025.\\nDigital Object Identifier 10.1 109/ACCESS.2025.3542562\\nExplainable Video Topics for Content Taxonomy:\\nA Multimodal Retrieval Approach to\\nIndustry-Compliant Contextual Advertising\\nWARUNA DE SILVA\\n , (Graduate Student Member, IEEE),\\nAND ANIL FERNANDO, (Senior Member, IEEE)\\nDepartment of Computer and Information Sciences, University of Strathclyde, G1 1XQ Glasgow, U.K.\\nCorresponding author: Anil Fernando (anil.fernando@strath.ac.uk)\\nABSTRACT Owing to the increased video content consumption in recent years, the need for advanced\\ncontextual advertising methods that leverage increasing user engagement and relevance on advertisement-\\nbased video-on-demand platforms has increased. Traditional behavior-based advertisement targeting is\\nwaning, particularly owing to the recent strict privacy policies that favor user consent and privacy. This\\nstudy proposes an innovative approach for integrating advanced natural language processing with multimodal\\nanalysis for video contextual advertising. To this end, transformer-based architectures, specifically\\nBERTopic, computer vision techniques, and large language models were used to extract sets of topics\\nfrom visual and textual video data automatically and systematically. The proposed framework decodes the\\ntaxonomy of content efficiently through videos in different levels of noise and languages. Empirical analysis\\nof the YouTube-8M dataset shows the potential for the approach to change the paradigm in video advertising.\\nBuilt to be scalable and easily adaptable, this solution can handle multifarious and complex user-generated\\ncontent well, suited for a wide range of applications across various media platforms.\\nINDEX TERMS Natural language processing, video contextual advertisements, multimodal fusion, topic\\nmodeling, BERTopic, contextual taxonomy standards, multi-label classification.\\nI. INTRODUCTION\\nAdvertisement-based video-on-demand (A V oD) has emerged\\nas a viable advertisement model owing to the significant\\nincrease in video consumption in recent years [1]. A V oD\\nplatforms earn revenue primarily through advertising via\\nreal-time bidding [2]. Several privacy-friendly advertisement\\nmeasurement and targeting methods have been developed\\nowing to the reduction in behavior-based advertisement tar-\\ngeting brought about by regulations such as the General Data\\nProtection Regulation (GDPR) and the California Consumer\\nPrivacy Act [3]. These laws advocate for an opt-in consent-\\nbased model, which encourages consumers to move towards\\nprivacy-preserving technologies. This has heightened the\\nrequirement for scalable methods capable of mapping media\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Zahid Akhtar\\n.\\nassets to industry taxonomies while ensuring that compliance\\nand monetization remain effective. Contextual advertising\\nhas emerged as a promising compromise. It operates via\\nbets placed on a cookie-less uncluttered environment and\\ndelivers advertisements perfectly suited to the type of content\\nbeing viewed based on semantic video indexing and deep\\nlearning processes [4]. Given that videos consist of rich\\nmultimodal data, such as audio, visual, and textual data, this\\nis a new research paradigm for multimodal approaches to\\nhandle various data types [5], [6]. Furthermore, integrating\\nsupport for multilingual enablement can extend reach,\\nallowing users to execute advertising strategies targeting all\\nrelevant languages and regions, without alienating potential\\naudiences [7].\\nSome studies have also suggested that thematically congru-\\nent advertisements improve both memorability and attitude\\ntoward advertisements in high-arousal contexts [8]. An in-\\nVOLUME 13, 2025\\n\\n 2025 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\\nFor more information, see https://creativecommons.org/licenses/by/4.0/ 30597'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 1, 'page_label': '30598'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\ndepth analysis was conducted to identify broader thematic\\nelements that are capable of making targeted video classifiers\\nmore efficient at adding value and engaging viewers. This\\napproach delivers higher memorability and positive viewer\\nattitudes using explicit and implicit contexts, thereby enhanc-\\ning the effectiveness of contextual advertising [9]. Beyond\\ncontextual advertising, topic modeling, which is a natural\\nlanguage processing (NLP) method, offers the possibility of\\ninferring explicit and implicit themes [10]. These topics are\\naligned with the content taxonomy more precisely to enable\\nfiner targeting in a larger advertising ecosystem.\\nIn this paper, we describe the utilization of transformer-\\nbased architectures, a family of models behind many recent\\nadvances in NLP and computer vision, to decode content\\ntaxonomies. Specifically, we employ distinct BERTopic\\nmodels to extract and analyze topics from both image\\nand textual data embeddings. These models systematically\\ninferred topics based on visual modalities and multilingual\\ntranscripts. In addition, we leverage large language models\\n(LLMs) to enhance the coherence and explainability of the\\ntopic representations. The inferred topics were systematically\\nmapped to pertinent taxonomies, enabling nuanced and\\ncomprehensive analysis of multimodal video data. This tech-\\nnique effectively handles varying noise levels across video\\nmodalities and flexibly identifies content taxonomy based\\non outcomes across multiple video modalities. It exhibits\\nboth flexibility and scalability, particularly when dealing\\nwith user-generated content, which is often inconsistent,\\nunstructured, and varies significantly in terms of quality.\\nThe deployment of this framework is expected to increase\\nthe precision and engagement efficiency of advertisements\\nsignificantly, effectively enabling programmatic advertise-\\nments on A V oD platforms. For this purpose, we performed\\nan empirical analysis of a semi-automated processed sample\\ndataset, YouTube-8M. Video content intrinsically involves\\nmultiple instances of a class occasionally; therefore, we used\\nmulti-label classifiers to address the complexity of video\\ndata comprising multiple types and categories. This study\\nrepresents an advancement in the field of modeling contextual\\nadvertisements and video topic modeling.\\nThe remainder of this paper is organized as follows.\\nRelated studies on contextual advertising and video topic\\nmodeling are discussed extensively in Section II. The\\nproposed methodology, including the data sources and\\nstructure, is described in Section III. The experimental and\\nanalytical results are presented in Section IV. The application\\nperspectives and directions of future research are discussed\\nin Section V. Finally, the paper is concluded in Section VI by\\nsummarizing the key contributions and insights of this paper.\\nII. RELATED WORK\\nThe increasing demand for A V oD services is evident as\\nmajor media players, such as Netflix, Rakuten, Discovery,\\nAmazon, and Comcast, have already launched or plan to\\nlaunch such services imminently [11], [12]. Personalized\\nadvertising approaches harness viewer data to display\\nadvertisements that are targeted and, therefore, not intrusive.\\nThis leverages viewers as influencers in the production of\\ncontent and the targeting of audiences while monetizing\\nviewer attention [13]. The GDPR and similar new regulations,\\nsuch as the upcoming Digital Services Act, are expected to\\ncomplicate the capture and use of personal data for targeted\\nadvertising by default [14]. One promising alternative is\\ncontextual advertising, which considers the context in which\\nadvertisement media are placed [4].\\nA. CONTEXTUAL ADVERTISING\\nVideoSense [15] was introduced for contextual advertise-\\nments on video platforms. It utilizes elements such as titles,\\ntags, queries, and local visual-aural features such as color,\\nmotion, and audio. Okada et al. [16] studied the process\\nof selecting advertisements for placement on videos. They\\nconsidered different forms of textual metadata created by\\nusers and stored them on a host webpage. These included\\ntitles, keywords, descriptions, categories, and comments, and\\nwere used to select relevant advertisements. This avoids the\\nneed to process images and videos elaborately. In Salad\\n[17], a convolutional neural network (CNN) was adopted\\nfor feature extraction and selection of the most salient\\nadvertisements. It aligns text with visual content, preserving\\nthe context using high-level features obtained from a deep\\nneural network for optimal relevance of advertisements in\\nonline videos. However, the aforementioned studies relied\\non metadata or a set of visual/aural cues without performing\\na proper analysis of the video content; therefore, they\\ndid not consider context-specific nuances that are essential\\nfor advertising. Similarly, Zhang et al. [18] conducted\\nresearch on online video advertising to optimize the balance\\nbetween advertisement intrusiveness and relevance. Their\\nwork incorporated a conventional histogram of oriented\\ngradient features for generic object detection and deep\\nCNNs for class-specific tasks, such as gender recognition\\nin clothing retrieval. Moreover, they considered and rec-\\nommended the incorporation of deep neural networks to\\nenhance object detection performance. Wang et al. [19]\\ncombined hue, saturation, and value color histograms with\\nOriented FAST and Rotated BRIEF(ORB) features to offer\\nusers a highly detailed measure of content similarity and,\\ntherefore, focused on the overall relevance of a scene rather\\nthan single objects in the scene. AI advances allow for\\ndeeper semantic analysis, enhancing ad placement. There\\nhave been three contextual factors in ad acceptance that\\nhave been studied: applicability, affective tone, and consumer\\nengagement [20]. An interesting step forward was made with\\nthe development of the multimodal approach, DEEP-AD\\n[21], which utilizes a temporal video segmentation algorithm\\nto relate advertisements with video content semantically.\\nThe algorithm segments videos into stories by analyzing the\\nvisual, audio, and semantic features using a battery of deep\\nCNNs. In DEEP-AD, the semantic descriptions of both video\\nscenes and advertisements are employed to ensure contextual\\nrelevance. Object and place recognition methods are applied\\n30598 VOLUME 13, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 2, 'page_label': '30599'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nto derive these semantic descriptions, thereby enhancing the\\nprecision of ad placement. Another contextual ad platform,\\nSemanticAd [22], extends a similar idea and aims at specific\\nad placement at semantic boundary positions in a video, not\\nsemantic compatibility alone. Unlike DEEP-AD’s semantic\\ndeep-description direction, SemanticAd utilizes story unit\\nextraction for ad and video segmentation and ad mapping in\\nterms of visual, audio, and semantic discontinuity. Further,\\nSong et al. [23] proposed a multimodal approach combining\\nvarious types of CNNs to extract multimodal features and\\nobtain a unified representation of over 140 movie video\\nclips based on semantics, objects, scenes, sentiments, colors,\\nand audio. They modeled topics using a semantics-based\\nmodel which is based on an I3D framework. The I3D\\nframework is suitable for the short shots used in recognition\\ntasks and precise action recognition. However, it does not\\nfunction effectively on long-range videos [24]. Existing video\\nsegmentation methods have predominantly targeted short\\nvideos characterized by clear visual changes and simple\\npatterns [25]. These unique properties of short videos can\\nbe learned using supervised configurations, yielding models\\nthat are highly resistant to the longer and more subtle\\nproperties of long-form video materials. NLP techniques\\nare relatively prevalent in traditional advertisement formats,\\nincluding search, social, web, and classified advertisements\\n[26]. Although a few studies considered the use of NLP in\\nvideo advertising until 2022 [26], recently, with the growing\\nimportance of video content, an interest in the exploration of\\nadvanced NLP concepts from multiple perspectives has been\\nobserved [20], [27]; hence, deeper insights with innovative\\napplications are expected in this domain. Finally, Explainable\\nAI (XAI) is becoming a critical issue in digital marketing,\\nin a direction to prevent a lack of transparency in AI-\\npowered ad placement. In [28], a model incorporates CTR\\nprediction, visual heatmaps, and LLM analysis for brands,\\nwith an objective towards providing increased transparency\\nin ad targeting. In this work, ad effectiveness is stressed\\nto be increased with interpretability in terms of audience\\nengagement through XAI.\\nB. VIDEO TOPIC MODELING\\nThe NLP method of topic modeling was primarily developed\\nfor text analysis, using popular models such as Latent\\nDirichlet Allocation (LDA), which employs probabilistic\\napproaches to discover hidden themes in large text corpora\\n[29]. In textual data, a topic is represented as a ‘‘bag\\nof words’’. For videos, a similar concept can be applied,\\nwith a ‘‘bag of features’’ representing the video content.\\nFrom a semantic perspective, a ‘‘topic’’ in video analysis\\nis represented by objects, behaviors, activities, events, etc.\\n[30]. To adapt LDA to videos, features extracted from each\\nframe may be quantized to the nearest visual words in a\\npredefined dictionary. However, unlike language models,\\nvideo analysis does not include predefined words. Therefore,\\na global embedding method such as word2vec [31] is not\\nsuitable, which leads to difficulties in semantic measure-\\nments. Although several approaches that combine language\\nand vision have partially addressed this issue, designing\\neffective information embedding methods for topic-based\\nvideo analysis remains challenging [32]. Although BERTopic\\n[33] was not designed specifically for video topics, its\\nfundamentals can be applied to analyze key images in videos\\nusing image embeddings and audio transcripts in conjunction\\nwith sentence transformers, because sentence semantics\\ncan capture more powerful and context-rich information\\nthan individual words. Recent works [34] have shown that\\nBERTopic outperforms other models, such as LDA [29], non-\\nnegative matrix factorization [35] and contextualized topic\\nmodels [36], thus placing it as the strongest candidate for\\nboth multimodal and textual topic analysis. This robustness\\nand semantic depth underpin its selection for video topic\\nmodeling in the present study.\\nIII. PROPOSED SYSTEM\\nInspired by recent neural network-based unsupervised\\napproaches to topic modeling, we propose a model that\\naccepts, as its input, a video with 1) video frames and 2)\\naudio transcripts as sentences to derive a representation of\\nboth visual and textual data. This method allows the model to\\nidentify and segment different topics in a video by defining\\nboundaries for thematic content and content changes. In this\\nmethod, all possible themes in a video are to be captured as\\ntopics, avoiding missing critical themes owing to the nature\\nof the video.\\nGiven: A video Xi with transcript Ai as a sequence\\nof sentences {s1, s2, . . . ,sn} and video frames Vi =\\n{k1, k2, . . . ,km}.\\nPredict: The set of taxonomies {µ1T1, µ2T2, . . . , µpTp}.\\nEach taxonomy is associated with a multiplier µi representing\\nterm frequency, i.e., the frequency and relevance of taxonomy\\nbased on the mapped topics.\\nThis framework consists of five major components,\\nas illustrated in Figure 1. The stages include 1. visual topic\\nmodel training, 2. textual topic model training, 3. feature\\nextraction, 4. video topic inference, and 5. industry content\\ntaxonomy association.\\nBoth visual and textual topic models are trained using\\nBERTopic [33], which leverages embeddings and hierarchi-\\ncal clustering to create coherent topic representations and\\nyields robust and interpretable topic models. These key visual\\nframes and audio transcripts are used as input in the video\\ntopic inference phase. Eventually, these topics are mapped to\\nthe IAB TechLab content taxonomy [37] to enable precise\\ncontextual advertising. This methodology provides a thematic\\nunderstanding of video content for contextual relevance.\\nA. TOPIC MODELING WITH BERTOPIC\\nTwo separate BERTopic pipelines were developed for visual\\ndata and audio transcripts based on the modular architecture\\nof BERTopic, which selects appropriate components to con-\\nVOLUME 13, 2025 30599'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 3, 'page_label': '30600'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nFIGURE 1. Multimodal embedding architecture for contextual video analysis.\\nfront the challenges presented by each modality. BERTopic\\nuses embeddings, dimensionality reduction, clustering, and\\ntopic-representation pipelines to generate coherent topics.\\nHyperparameter tuning is a significant requirement for\\nmapping a model with the characteristics of each modality\\nto achieve coherence topics from each stream.\\nFor each video, Xi, we initially partition the data into visual\\nand audio components, enabling the use of tailored models for\\neach modality.\\n1) COMMON COMPONENTS OF BERTOPIC\\n• Embedding: Input data were transformed into rich\\nfeature sets using advanced embedding models. These\\nembeddings capture the essential characteristics of the\\ndata, enabling effective subsequent processing.\\n• Dimensionality Reduction: The fine-tuning of the Uni-\\nform Manifold Approximation and Projection (UMAP)\\n[38] hyperparameters during dimension reduction bal-\\nances the simplicity of the data while preserving the\\ncomplex structures corresponding to each modality.\\n• Clustering: Fine-tuned hyperparameters were provided\\nfor Hierarchical Density-Based Spatial Clustering of\\nApplications with Noise (HDBSCAN) [39] to provide\\nclear and meaningful visual clustering, facilitating the\\ndiscovery of heterogeneous thematic content.\\n• Topic Representation Formation: Topic vectors\\n{TVi1 , TVi2 , . . . ,TVin } and {TAi1 , TAi2 , . . . ,TAin } are\\ncreated for visual and audio data, respectively.\\nIn BERTopic, to obtain an accurate representation of\\nthe topics from the bag-of-words matrix, the term\\nfrequency-inverse document frequency (TF-IDF) [40]\\nis adjusted to work on the cluster level instead of the\\ndocument level. This adjusted TF-IDF representation\\nis called c-TF-IDF, and it considers the essential\\ndifferences between documents in different clusters:\\nwx,c = tfx,c × log\\n(\\n1 + A\\nfx\\n)\\n(1)\\nwhere:\\ntfx,c = frequency of word x in class c\\nfx = frequency of word x across all classes\\nA = average number of words per class\\n2) BERTOPIC(VISUAL DATA)\\nAs depicted in Figure 2, the following steps are tailored for\\nvisual modality.\\n• Embedding with CLIP-ViT [41]: The ‘‘CLIP-ViT-B-\\n32’’ embedding model is deployed to transform images\\ninto a rich feature set that is encoded using a Vision\\nTransformer (ViT) with a base-size (B) architecture and\\n32 attention heads.\\nModels such as the ViT [42], ResNet [43] and VGG-\\n16 [44] tend to excel when it comes to visual features\\nextracted by clustering. However, these traditional meth-\\nods mostly fail [45] to deliver semantically meaningful\\nclusters even while being quite effective in clustering\\nvisually similar images with approaches such as Nearest\\nNeighbor Matching [46], [47]. CLIP tries to overcome\\nthis limitation by aligning visual and textual data in\\na common feature space, thus enabling clustering that\\ncaptures both the visual and semantic relationships.\\nCLIP has already outperformed over 20 state-of-the-\\nart visual-model-based methods such as ResNet [45].\\nWith ViT embeddings and semantic richness, CLIP-\\nViT ensures that clustering achieves both robustness and\\nmeaningful grouping [48]. This dual capability enhances\\n30600 VOLUME 13, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 4, 'page_label': '30601'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nFIGURE 2. Visual data processing workflow using BERTopic.\\naccuracy and adaptability for complex and nuanced\\ntasks, solidifying its advantage over other technologies.\\n• Image Captioning: The ‘‘vit-gpt2-image-captioning’’\\nmodel is used to obtain textual representations of the\\nimages, translating the visual details into descriptive\\nlanguage, thereby bridging the gap between visual\\nfeatures and textual analysis. The ViT-GPT2 model\\ncombines ViT architecture [42] and a pre-trained GPT-\\n2 language model [49] for generating image captions;\\nrecent implementations are given by Hugging Face [50].\\n3) BERTOPIC(AUDIO DATA)\\nFor the audio component shown in Figure 3, the following\\nprocess is adopted:\\nConcurrently, for the audio component Ai of each video Xi,\\n• Embedding with Multilingual Model [51]: We\\nuse a multilingual embedding model to transform\\naudio transcripts into a rich feature set. By utilizing\\nparaphrase-multilingual-MiniLM-L12-v2, a multi-\\nlingual SBERT [52] variant, we embed the data\\neffectively while preserving the linguistic nuances and\\ncontext inherent in the transcript. Unlike BERT [53]\\nand RoBERTa [54], these models generate high-quality\\nsentence embeddings directly, with no complex pooling\\nmechanism needed [52]. Meanwhile, BERT [53] is\\nmonolingual, and while mBERT [55], though supportive\\nof 104 languages, is not optimized for sentence-level\\ntasks; hence, these are performing so poorly and leading\\nto such high latency. MiniLM-L12-v2, on the other hand,\\nsupports over 50 languages [52] and can capture nuances\\nin different linguistic variations in multilingual contexts\\nFIGURE 3. Audio data processing pipeline with BERTopic.\\nand hence can handle multilingual transcripts. These\\nreasons make it an ideal choice for our framework,\\nsince it gives a good balance between accuracy and\\nmultilingual efficiency.\\nB. TOPIC EXPLAINABILITY\\nTopic representations were refined using c-TF-IDF to\\nimprove the accuracy and alignment of candidate topics\\nfor content taxonomy mapping. To further enhance topic\\nexplainability, we conducted evaluations LLMs to assess\\nthe coherence and interpretability of the generated topics\\nensuring stronger alignment with the taxonomy mapping.\\nWe provide prompts in the following customized form:\\nprompt = \"\"\"\\nI have a topic that is described by the\\nfollowing keywords: [KEYWORDS]\\nIn this topic, the following documents\\nare a small but representative subset\\nof all documents in the topic:\\n[DOCUMENTS]\\nBased on the information about the topic\\nabove, create a short description of\\nthis topic with few words.\\n\"\"\"\\nwhere, particular parameters of the template for the\\nprompt are ‘‘[KEYWORDS]’’ and ‘‘[DOCUMENTS]’’.\\n‘‘[KEYWORDS]’’ is to be replaced with specific terms rep-\\nresenting the topic, whereas in place of ‘‘[DOCUMENTS]’’,\\na selection from all the documents representative of the\\ntopic is to be used. With respect to the visual modality,\\n‘‘[DOCUMENTS]’’ refers to the image captions generated\\nfrom the images within each topic.\\nVOLUME 13, 2025 30601'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 5, 'page_label': '30602'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nC. VIDEO FEATURE EXTRACTION\\nVideo features were extracted by processing audio compo-\\nnents, which were obtained from audio transcripts, and visual\\ncomponents, which were obtained from keyframes. Both\\nfeatures were transmitted to textual and visual models for\\ntopic inference.\\nFor visual data, we extracted keyframes using FFmpeg\\n[56] at a rate of one frame per second (fps = 1) with a\\nsimilarity threshold exceeding 0.45, ensuring the uniqueness\\nof the captured frames. This method captures the essential\\nvisual dynamics of video content effectively. From the visual\\ncontent of Xi, we extracted key images, Vi.\\nFor audio data, OpenAI’s Whisper model [57] was used\\nto obtain accurate multilingual transcripts, with MP4 files\\nprepared using FFmpeg for format compatibility. From the\\ntextual content of Xi, we extracted the audio transcript, Ai.\\nD. VIDEO TOPIC INFERENCE\\nThe goal of this step is to pipe through each Xi with\\nboth visual and textual BERTopic modeling to get the set\\nof topics identified as {T1, T2, . . . ,Tn}. Each Xi consists\\nof multimodal components: Vi (visual data—key frames)\\nand Ai (audio data—multilingual transcripts). The inference\\nprocess predicts the topic distribution for each Xi, assigning\\na set of probable topics based on previously learned topic\\nrepresentations.\\nThe predicted topic distribution for the i-th video sample\\ncan be expressed as:\\nTopic Distribution for Xi = {(Tk , pi,k ) | k = 1, 2, . . . ,n}\\n(2)\\nwhere:\\n• Tk : The k-th topic, represented by a high-level descrip-\\ntive label generated by a LLM, summarizing the primary\\ntheme or concept of the topic.\\n• pi,k : The probability of the k-th topic for sample Xi,\\nindicating the relevance of Tk in describing Xi. Higher\\nprobabilities suggest a stronger alignment between Xi\\nand topic Tk .\\nThis probability-based representation enables an inter-\\npretable assignment of topics to new data samples, driven\\nfrom both the visual and textual information. This approach\\nenables the broad understanding of high-level descriptive\\nlabels for each topic, which characterizes major themes\\npresent in the video sample without referring to individual\\nterms within each topic.\\nE. TAXONOMY MAPPING\\nIn the final step of our pipeline, explainable topics were\\nmapped to their closest semantic representations based on\\nrelevant items in our content taxonomies. Each entry in the\\ntaxonomy [37] includes structured hierarchical information\\nacross Tier 1, Tier 2, Tier 3, and Tier 4, listed in separate\\ncolumns for each row. For the purposes of semantic mapping,\\nwe first concatenated these tiers for each row into one\\nhierarchical keyword string, thus capturing the full semantic\\ncontext of the taxonomy.\\nFor every row i, the concatenated taxonomy string Ci is\\nobtained by concatenating nonempty tier values separated by\\na space:\\nCi = trim\\n\\uf8eb\\n\\uf8ed\\nn∑\\nj=1\\n(\\nTi,j + ‘‘′′)\\n· I(Ti,j ̸= ‘‘’’)\\n\\uf8f6\\n\\uf8f8 (3)\\nwhere:\\n• Ti,j: The taxonomy term at row i and tier j.\\n• n: The total number of tiers (e.g., n = 4 for Tier 1 to Tier\\n4).\\n• I(Ti,j ̸= ’’’’): An indicator function equal to 1 if Ti,j is\\nnon-empty, and 0 otherwise.\\n• +‘‘′′: Represents the addition of a single space after each\\ntier Ti,j.\\n• trim: A function that removes any trailing whitespace in\\nthe final concatenated string.\\nThis formula, the tiers are concatenated in sequence from\\nTier 1 through Tier n, in the order of their indices. Further,\\nall the empty tiers are excluded, Ti,j =′′′′ in the course of the\\nconcatenation. Then, a space is added after each non-empty\\ntier to separate the terms in the course of concatenation, and\\nthe function ‘trim()‘ is applied to remove those dispensable\\nblank spaces at the end of the concatenated result. The output\\nCi is therefore a whitespace-free, concatenated string of non-\\nempty tiers from row i. This method ensures that Ci is well-\\nformed, free of redundant spaces, and accurately represents\\nthe concatenated terms for the given row.\\nEach of them combines the tiers and uses the resulting\\nstrings, Ci, in a semantic similarity check against explainable\\ntopics. This will, in turn, enable the mapping of each topic\\nto the closest content taxonomy entry according to semantic\\nproximity, hence enabling appropriate topic categorization.\\nThis is computed in terms of cosine similarity, which\\nmeasures the cosine of the angle between two vectors. The\\ncosine similarity between a topic vector Tk and a taxonomy\\nvector Ci is defined by:\\ncosine_similarity(Tk , Ci)\\n= Tk · Ci\\n∥Tk ∥∥Ci∥,\\nwhere cosine_similarity(Tk , Ci) ≥ θ (4)\\nwhere:\\n• Tk : The k-th topic vector.\\n• Ci: The i-th taxonomy vector.\\n• θ: A predefined cosine similarity threshold, which filters\\nout mappings with scores below θ.\\n• Tk · Ci: The dot product of the vectors Tk and Ci.\\n• ∥Tk ∥ and ∥Ci∥: The magnitudes of the vectors Tk and\\nCi, respectively.\\nFinally, the predicted outcome of taxonomies is:\\n{µ1T1, µ2T2, . . . , µpTp} (5)\\n30602 VOLUME 13, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 6, 'page_label': '30603'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nTABLE 1. Characteristics of the multimodal video analysis dataset.\\nThis outcome is obtained based on given term frequencies\\nfor each taxonomy. Each taxonomy is associated with a\\nmultiplier µi representing term frequency. The multiplier\\nµi indicates the frequency and relevance of the taxonomy\\nbased on the mapped topics. In this process, we also used\\nthe topic probabilities pi,k derived from Equation (2) during\\nthe process of inference and had a threshold to retain only\\nhigher-probability topics to map onto taxonomies. For the\\nexperiments, it was set to 0.7, where 1 is the highest\\nconfidence. The taxonomy frequencies are then counted\\nfrom these filtered topics, making sure only high-confidence\\nmappings contribute to the final results. This is a parameter\\nthat can be set flexibly within the framework to allow a\\ntrade-off between quality and coverage in the output. These\\nprobabilities represent the taxonomy weights within the\\nframework.\\nIV. EXPERIMENTS\\nA. TEST DATA\\nTwo distinct datasets were used for training and evaluation.\\nThe training dataset, a very rich text corpus, was used to train\\nand fine-tune the textual topic model. A corresponding set\\nof images was used to train and fine-tune the image topic\\nmodel. The second dataset comprised real-world videos that\\nwere used to evaluate the proposed methodology for video\\ntaxonomy analysis. Subsequently, fine-tuned models with a\\nrich text corpus and image dataset were applied to a real\\nvideo dataset to identify and analyze different topics. Model\\ntraining was focused on the food domain using specialized\\ndatasets. For fine-tuning using text, a dataset comprising\\n180,000 recipes from Food.com obtained from Kaggle was\\nused for a pretrained BERTopic model. In the case of images,\\nthe Food101 dataset, consisting of 50,000 images, was used\\nto train a multimodal BERTopic model. The model was tested\\non YouTube-8M [58], a dataset comprising videos uploaded\\nby users and labeled using the ground truth data. One hundred\\nand thirty-five videos were sampled in the food category as\\ntraining data, as described in Table 1. These videos contained\\nvarying levels of noise, languages, and spacings of time\\nintervals. Wherever necessary, the ground-truth labels were\\nfurther refined by fixing the accuracy of the updated ground-\\ntruth labels. Two experts were enlisted in a user study to\\nassess and select the relevant advertisement taxonomy rows\\nassociated with the video content. Each ground-truth label\\nwas mapped to a particular row in the taxonomy [37], T1,\\nT2, T3, and T4.\\nB. EVALUATION METRICS\\nMeasuring the number of topics in videos using multi-label\\nclassification is essential because a video generally involves\\nmore than one category. This has motivated considerable\\nresearch on multi-label classification [59]. Important eval-\\nuation measures include Hamming loss, subset accuracy,\\nprecision, recall, and F1-score [59], [60].\\nIn multi-label classification, each instance belongs to\\nmultiple classes simultaneously. Each binary label can be\\nconsidered to be a vector y ∈ {0,1}L , where 1 represents\\nthe existence of a specific label from a predefined set Y =\\n{λ1, . . . , λL }, while 0 represents the opposite. For a dataset\\nD = {x1, x2, . . . ,xn} consisting of n videos, we consider\\nthe task of learning the classifier h : X → Y that\\nproduces any input in the appropriate sets of labels. In simple\\nterms, h(x) provides a subset of preselected labels for each\\ninput, considering an instance with multiple labels. Hamming\\nloss is defined to be the fraction of the number of labels\\npredicted incorrectly. It provides an overall error rate for the\\nclassification system [61]. Hamming loss is given by\\nHamming Loss = 1\\nN · L\\nN∑\\ni=1\\nL∑\\nj=1\\n1{yij̸=ˆyij} (6)\\nwhere N denotes the number of samples, L denotes the\\nnumber of labels, yij denotes the true value of the j-th label\\nfor the i-th sample, ˆyij denotes the predicted value of the j-th\\nlabel for the i-th sample, and 1 denotes the indicator function\\nthat returns 1 if yij ̸= ˆyij and 0 otherwise.\\nSubset accuracy is defined to be the number of correctly\\npredicted labels divided by the total number of labels, with a\\npredicted set counted as correct only if it is an exact match of\\nan actual set [62]. It is defined as follows:\\nSubset Accuracy =\\n∑N\\ni=1 1{yi=ˆyi}\\nN (7)\\nwhere yi denotes the true label for the i-th instance and ˆyi\\ndenotes the predicted label for the i-th instance.\\nWe use the micro F1-score to optimize the overall matching\\nbetween content taxonomies in the videos. As the micro\\nF1-score considers the combination of all labels, it is well\\nsuited in scenarios with large numbers of labels, and therefore\\nrepresents classifier performance effectively [63].\\nMicro-Precision, Micro-Recall, and Micro F1-score:\\nMicro_precision =\\n∑q\\nj=1 tpj\\n∑q\\nj=1 tpj + ∑q\\nj=1 fpj\\n(8)\\nMicro_recall =\\n∑q\\nj=1 tpj\\n∑q\\nj=1 tpj + ∑q\\nj=1 fnj\\n(9)\\nMicro_f1 = 2 × micro_precision × micro_recall\\nmicro_precision + micro_recall\\n(10)\\nwhere:\\n• tpj: True positives for the j-th class.\\n• fpj: False positives for the j-th class.\\nVOLUME 13, 2025 30603'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 7, 'page_label': '30604'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nTABLE 2. Optimal parameter values for topic models.\\n• fnj: False negatives for the j-th class.\\n• q: Number of classes.\\n• Micro_precision: Micro-averaged precision across all\\nclasses.\\n• Micro_recall: Micro-averaged recall across all classes.\\nGiven by Equations( 8), (9) and (10), these metrics provide\\nan aggregated measure of performance across all classes by\\ntreating each instance equally irrespective of its class.\\nC. EXPERIMENTAL CONFIGURATIONS: TAXONOMY\\nRETRIEVAL\\nThis involved the tuning of hyperparameters that would\\nenable the training of topic models on the textual and\\nvisual content, thus providing the first set of key steps for\\nour experiments. These parameters were iteratively adjusted\\nto capture optimal topic coherence. To ensure a balanced\\nevaluation and avoid overfitting to coherence scores, we fine-\\ntuned BERTopic’s hyperparameters to optimize both human-\\nperceived semantic coherence [64] and diversity in topic\\nclusters.\\nThe methodology that was followed consisted of various\\nruns with different hyperparameter settings of UMAP\\nand HDBSCAN for dimension reduction and clustering,\\nrespectively, systematically attempted to test the effect\\nthey produced on the interpretability and the quality of\\ntopics generated. UMAP parameters are: n_neighbors,\\nn_components, min_dist, and metric. A list\\nin HDBSCAN also includes min_cluster_size,\\nmin_samples, and metric. to tune the cluster for\\nrefinement and stability.\\nThe optimal parameters of the text topic model and the\\nvisuals topic model, as shown in Table 2, were informed by\\nthe nature of the datasets.\\nThis might be explained by the complementary nature\\nof the diverse text corpus and dense image dataset used\\nin this study. Whereas the text corpus provides semantic\\nrichness and variation that enables embeddings to capture\\na wide range of thematic nuances, the dense images ensure\\nthat the features are represented in detail, hence fine-grained\\nclustering.\\nThese configurations have balanced interpretability and\\nclustering quality, which meets the model output require-\\nments of the experimental objectives and real-world tasks.\\nFigure 4 summarizes findings for the textual topic model,\\nFIGURE 4. Topic distribution for audio-based model.\\nFIGURE 5. Topic distribution for visual-based model.\\nwhile Figure 5 summarizes the findings of the visual topic\\nmodel. Both figures illustrate the topic distribution at an\\noptimal point that reflects the coherence achieved with the\\nbest configuration for each modality. The found topics were\\nexplained with the support of a LLM. In our approach\\nto amplifying LLM selection accuracy, the usage of two\\ndifferent LLMs in our approach was considered:\\n• T5-Base model [65]\\n• PaLM 2 model [66]\\nOnce the model training was complete, the subsequent task\\nwas to apply the trained topic models to infer topics from\\na dataset of 135 selected videos. Additionally, the inferred\\ntopics had to be mapped to the predefined content taxonomies\\nsuch that they would align with the overall thematic structure.\\nWe also experimented separately with the unimodal and\\nmultimodal methods. These tests enable the comparison of\\nmodel performance across unimodal and multimodal data and\\nset baselines for further analysis.\\n• Unimodal Data\\n– Audio Transcripts data only\\n– Visual data only\\n• Multimodal Data\\n– Combination of audio transcripts and visual data\\nIAB content taxonomy [37] are divided into several levels\\nof categorization, namely T1, T2, T3, and T4. Therefore,\\nour experimental design has tried to test the performance of\\nthe framework at the higher-order(T1) and finest-grade(T4)\\nlevels of the taxonomy spectrum. The results for top-level\\ntaxonomies are shown in Table 3, while in Table 4, the results\\nfor granular-level taxonomies are given.\\n30604 VOLUME 13, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 8, 'page_label': '30605'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nTABLE 3. Top-level taxonomies inference results using pre-trained topic models configured with PaLM 2 and T5-Base.\\nTABLE 4. Granular-level taxonomies inference results using pre-trained topic models configured with PaLM 2 and T5-Base.\\nD. EXPERIMENTAL CONFIGURATIONS: ROBUSTNESS OF\\nNOISES IN MODALITIES\\nTo ensure that the evaluation of the proposed framework is\\nholistic, one such perspective is constituted by the robustness\\nof the framework under real-world noisy conditions in both\\nvisual and transcript modalities. YouTube-8M dataset, which\\ninherently varies to a great degree and is noisy in nature due\\nto being generated by users.\\n• Visual Noise: Irrelevant frames, misaligned content,\\nor videos with very short durations and few represen-\\ntative frames.\\n• Noisy Language: The incomplete transcripts or back-\\nground music or noise interferes with the speech.\\n• Noisy Combination: Scenarios where the visual\\nand transcript modalities are both noisy at the\\nsame time.\\nIn particular, we conducted experiments on a curated\\nsubset of 30 videos representing these noise categories\\nfrom the YouTube-8M dataset. The performance of inferring\\nT1 and T4 taxonomies from videos selected for the noise\\ncategories is presented in Tables 5 and 6, respectively.\\nE. EXPERIMENTAL CONFIGURATIONS: ROBUSTNESS TO\\nLANGUAGE-SPECIFIC SCENARIOS\\nFurther analysis with respect to the performance of the\\nproposed framework under language-specific conditions is\\nperformed by dividing the selected 80 videos into subsets\\nbased on the primary language of the transcript. The videos\\nare categorized as:\\n• Visuals with English Transcripts: Videos for which the\\ncontent of the transcript is completely in English and\\nvisually represented.\\n• Non-English Transcripts with Visuals: Videos whose\\ntranscript content is mainly composed of non-English\\nlanguages but come with visual data.\\nThis division enables a focused evaluation of the frame-\\nwork’s performance across distinct language-specific scenar-\\nios, considering both the availability and quality of transcript\\ndata. The results for the T1 and T4 taxonomies, reflecting the\\neffectiveness of multimodal fusion under these conditions,\\nare reported in Tables 7 and 8\\nV. RESULT ANALYSIS AND DISCUSSION\\nThis study assesses the efficiency of the proposed frame-\\nwork by quantifying its ability to generate video explain-\\nable topics that can be mapped to content taxonomies.\\nThese are highlighted in terms of semantic accuracy,\\nstrength against noisy conditions, versatility across lan-\\nguages, and qualitative benchmarks against state-of-the-\\nart methods appealing for scalability and industrial-grade\\nimplementation.\\nA. SEMANTIC ACCURACY IN VIDEO TOPIC MODELING\\nThis reflects that the framework infers the topics of a video\\nin a semantically accurate way and maps them to content\\ntaxonomies, as reflected by Tables 3 and 4.\\n1) TOP-LEVEL TAXONOMIES\\nFrom the two sets of evaluation configurations from the\\ntopic modeling framework where either PaLM 2 [66] or T5-\\nBase [65] was used for topic labeling the performance is\\ngreatly improved by combining transcript and visual inputs.\\nThe highest F1-score and subset accuracy are 0.80 and\\n0.66, respectively, when using the PaLM 2-configured model.\\nWhereas the T5-base-configured model achieves an F1-score\\nof 0.76 and a subset accuracy of 0.54. The main contribution\\nseen here, particularly of large-parameter models such as\\nPaLM 2, tends to result in semantically richer and more\\ninterpretable topic labels that lead to better overall framework\\nperformance.\\n2) GRANULAR TAXONOMIES\\nAt the level of more granular fine-grained distinction and\\nhence more challenging, results of the PaLM-2-configured\\ntopic model are the F1-score at 0.61, subset accuracy\\nequal to 0.22; while slightly outperforming is T5-Base-\\nconfigured with scores 0.59 and 0.23 correspondingly. At this\\nlevel, performance decline is reflected by the fact that no\\ndetailed topic representation exists in these models trained;\\nhence, further training over more fine-grained data is highly\\nrequired to have better coverage of topics. Besides, for\\nsuch a topic modeling task with high accuracy, one can\\nconsider hierarchical topic modeling (HTM) [67]. HTM can\\nVOLUME 13, 2025 30605'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 9, 'page_label': '30606'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nTABLE 5. Top-level taxonomies inference results under noise conditions using pre-trained topic models configured with PaLM 2 and T5-Base.\\nTABLE 6. Granular-level taxonomies inference results under noise conditions using pre-trained topic models configured with PaLM 2 and T5-Base.\\nTABLE 7. Top-level taxonomies inference results using transcript language with pre-trained topic models configured with PaLM 2 and T5-Base.\\nTABLE 8. Granular-level taxonomies inference results using transcript language with pre-trained topic models configured with PaLM 2 and T5-Base.\\nallow finer grainedness and hence better granularity and\\nexplainability.\\nMoreover, these results partly reflect the limitations\\nof using subset accuracy as a measure for multi-label\\nclassification. This measure, since it represents only cases\\nwhere all the taxonomies are predicted correctly, does not\\naccount for partially correct predictions, which results in the\\nsubset accuracy generally assuming low values [59], [62].\\nThis is particularly reflected in our results at the more difficult\\ngranular level, where an exact match is often challenging to\\nget. The F1-score provides a better balance in evaluation for\\nthese models.\\nB. ROBUSTNESS UNDER NOISY CONDITIONS\\nThe robustness of the framework under different noisy\\nconditions, as shown in Tables 5 and 6, is crucial\\nfor real-world applications where data quality is highly\\nvariable.\\n1) TOP-LEVEL TAXONOMIES\\nOn noisy visuals and clean transcripts, PaLM 2-configured\\nmodel achieves a F1-score of 0.75 compared with the\\nbaseline score of the T5-base-configured model at 0.68.\\nSimilarly, for noisy transcripts with clean visuals, the best\\nperformance by the PaLM 2-configured model secures an F1-\\nscore of 0.77 outperforming the T5-base-configured model\\nwhich performed at 0.71. In the most challenging case, both\\ntranscripts and visuals are noisy the PaLM 2-configured\\nmodel achieves an F1-score of 0.53, showing that it can\\nmaintain reasonable performance even when features are\\nseverely limited and noisy. This is slightly better compared to\\nthe T5-base-configured, which records an F1-score of 0.51 in\\nthe same setting.\\n2) GRANULAR TAXONOMIES\\nNoise is most evident at the granular level, where fine-grained\\ndifferentiations are more sensitive to degradation in data.\\nIt can be observed that for the noisiest condition, both noisy\\ntranscripts and noisy visuals- PaLM 2-configured model\\noutperforms at 0.28 while the T5-base-configured model at\\n0.23 F1-score. The performance decline at this level reflects\\nnot just the presence of noise but also that this will have a\\nripple effect due to the unavailability of detailed topic rep-\\nresentations in the trained models, as discussed earlier. This\\nlimitation is further influenced by the nature of YouTube-\\n8M videos, which are very short in duration, thus providing\\n30606 VOLUME 13, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 10, 'page_label': '30607'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nlimited data for extracting detailed features under the given\\nshort time span of the videos. However, since the framework’s\\nprimary target is identifying ad opportunities, and real-world\\nads are often positioned after longer content, this issue is\\nlikely to be mitigated in real-world conditions, even at a\\ngranular level. Despite the overall decrease in accuracy, the\\nframework retains reasonable inference capabilities, high-\\nlighting its resilience under the given short time span of the\\nvideos.\\nC. MULTILINGUAL ADAPTABILITY\\nAs shown in both Tables 7 and 8, the framework handles\\nmultilingual input effectively by showing adaptability across\\nnon-English transcripts in addition to English.\\n1) ENGLISH TRANSCRIPTS\\nThe best F1-scores are obtained in the case of English\\ntranscripts when supported by visuals, with up to 0.83 at\\nthe top level, and 0.66 at the granular level, due to the\\nPaLM 2-configured model. These results reflect the inherent\\noptimization that this framework has undergone from primary\\nlanguage, particularly English.\\n2) NON-ENGLISH TRANSCRIPTS\\nThis demonstrate that it will handle non-English transcripts\\nwell, yielding an F1-score of 0.79 at the top level and\\n0.59 at the granular level when using the PaLM 2-configured\\nmodel. Thus, the system works relatively well on the non-\\nEnglish transcripts, which proves that this approach could be\\nmore adaptable with regards to multilingual data with slight\\ndegradation in performance.\\nD. GENERALIZATION ABILITY OF THE FRAMEWORK\\nAlthough the current study focuses on a smaller subset\\nof IAB Tech Lab’s content taxonomies, this subset effec-\\ntively illustrated the capabilities of the proposed approach.\\nNotably, the embedding-based approach of BERTopic applies\\nincremental training to integrate new topics [68] and\\nis therefore naturally scalable for real-world applications\\nwhere new taxonomies have to be inferred by incremental\\ntopic modeling. This reduces the memory needed for\\ntraining a topic model. For instance, the tested method\\ncould scale to cover the full spectrum of IAB Tech\\nLab’s 700+ content taxonomies [69] by processing the\\ndata in manageable batches. This scalability is achieved\\nby beginning with initial training on a baseline dataset\\nusing the fit() method but supports partial_fit()\\nupdates incrementally [70], thus allowing dynamic learning\\nwithout full retraining. It remembers previously learned\\ntopics while refining or introducing new topics as the data\\nevolves.\\nFurthermore, for improved explainability of topics and\\ngeneration of descriptions, the inclusion of models such as\\nPaLM 2 shows reasonable performance gains over T5-Base,\\nwhile commercially available LLMs like OpenAI’s GPT-\\n4 hold even higher promise. While these high-parameter\\nLLMs are expensive to use for inference, our framework\\nrestricts their usage to training time, making these inference\\ncosts irrelevant for deployment. The approach that makes\\neffective use of such advanced LLMs during training is to\\nuse the models saved by these in deployment to come up\\nwith accurate and interpretable topic descriptions, improving\\ntaxonomy inference without having to use commercial APIs\\nconstantly.\\nOnce the model is trained with fine tuned parameters,\\nthe inference focuses on two important parameters: topic\\nprobabilities pi,k from Equation (2) and cosine threshold θ\\nfrom Equation (4). Whereas lower taxonomy weights will\\nallow coverage for niche themes, higher weights will prior-\\nitize dominance of topics, hence precision. Similarly, lower\\ncosine thresholds widen the matching with diverse content\\nfor wider matching. For instance, general video platform sets\\nlower weights and thresholds for broader coverage, while a\\nspecialized provider might want to use higher thresholds for\\nmore precision. In our experiments, setting both parameters\\nto 0.7 resulted in an approach that balanced good coverage\\nwith reasonable confidence. This underlines the flexibility of\\nthe framework to adapt-from general platforms to providers\\nfocused on niches-which shows scalability in real-world\\neffectiveness.\\nE. COMPUTATIONAL EFFICIENCY DURING INFERENCE\\nBecause the platform can be extended to process the\\nvideos in batches, rather than taking the whole video as\\nan input and analyzing it, the computational demands for\\nthe inference of long-form video content are minimal.\\nSection V-G discusses this further in detail. This aligns\\nseamlessly with the requirements for ad placements since,\\non video platforms, ads are placed with a structured frequency\\nto maintain reasonable time blocks. These blocks can be\\nanalyzed independently; hence, computational overheads\\nwill be reduced and efficiency enhanced. Also, a platform\\ncomponent selection method opted for this purpose provides\\nits full support while maintaining accuracy uncompromised.\\nWe adopt the multilingual variant of MiniLM [52] for\\ntranscripts, which has a lightweight architecture that ensures\\ncomputational efficiency [52]. Its latency is significantly\\nreduced compared to BERT and mBERT, while still yielding\\nrobust performance for multilingual semantic tasks in terms\\nof bi-text retrieval [52]. For keyframes, we adopt the CLIP-\\nViT-B/32 model for strong performance balanced with speed.\\nFurther, recent improvements, such as Distill-ViT-B/32, have\\noffered improved embedding efficiency using significantly\\nfewer resources [71], although we have not applied this model\\nyet; this might be a promising direction for future work.\\nIn the direction of model inference efficiency, serialization of\\na topic model by the safetensors [72] format minimizes model\\nload times while ensuring safety upon deployment [73].\\nThe experiments were conducted on an AWS SageMaker\\nnotebook instance of type ml.t3.xlarge, which has\\n4 vCPUs and 16 GB of memory. This setup was good enough\\nVOLUME 13, 2025 30607'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 11, 'page_label': '30608'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nTABLE 9. Qualitative comparison of semantic analysis components.\\n30608 VOLUME 13, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 12, 'page_label': '30609'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nFIGURE 6. Evolution of content taxonomy over time in the video.\\nto perform inference on a 7-minute video for about 3-5\\nseconds of processing time.\\nHowever, Conventional feature extraction from videos,\\nkeyframe generation, and transcription are computation-\\nally costly tasks. Hence, our optimization in keyframe\\ngeneration includes the selection of unique frames with\\na given threshold of similarity 0.45, while our choice is\\nWhisper [57], a lightweight yet accurate multilingual ASR\\nmodel. These optimizations streamlined the preparation of\\nthe input through a reduction in size and complexity of the\\ndata and thus allow for efficient and scalable inference at\\nminimal computational costs.\\nF. CONCEPTUAL COMPARISON OF SEMANTIC ANALYSIS\\nMETHODS FOR CONTEXTUAL ADVERTISING\\nThis section provides a conceptual comparison of the\\nproposed framework against past works in contextual adver-\\ntising. In order for the comparative analysis to be useful,\\nattention has been restricted to purely semantic-related\\nanalysis components of the selected past works, given their\\ndirect linkage with the aims of the study. Other factors,\\nsuch as sentiment analysis or additional auxiliary features\\nconsidered in prior studies, are outside the scope of this\\ncomparison.\\nThe key aspects of semantic understanding, including\\nrobustness to noise, explainability, scalability, and multilin-\\ngual capability, are analyzed conceptually and illustrated in\\nTable 9. These aspects are critical to the proposed frame-\\nwork’s ability to deliver explainable, industry-compliant,\\nand globally adaptable contextual advertising solutions.\\nBy comparing these elements conceptually, the analysis\\ndemonstrates how the proposed approach advances the state-\\nof-the-art methodologies in semantic analysis for contextual\\nadvertising.\\nG. BROADER FOCUS OF UPCOMING STUDIES\\nThe explainable video topics for content taxonomy frame-\\nwork opens up avenues for a refined contextual advertising\\nsolution for long-range videos. By effectively tracking\\ncontent taxonomies over time, advertisers can dynamically\\nalign advertisements with evolving video themes. To address\\na key challenge in long-form video analysis specifically topic\\ndrift where themes evolve or shift over time the framework\\nsegments videos into smaller, time-bound units. The initial\\nexperiment illustrates that the method of segmentation pre-\\nserves the coherence of the topic within segments and support\\nthe topic drift in longer videos. Furthermore, this strategy\\naligns with the computational optimizations discussed in V-E,\\nenhancing scalability and efficiency while allowing dynamic\\nadaptation to evolving content.\\n1) FEASIBILITY SETUP AND INITIAL FINDINGS\\nConsider a video Xi with the audio transcript Ai, consisting\\nof a sequence of phrases {p1, p2, . . . ,pm} along with their\\nrespective start time codes {start1, start2, . . . ,startm} and\\nend time codes {end1, end2, . . . ,endm}, as well as keyframes\\n{k1, k2, . . . ,kn} extracted at specific time intervals within\\nthe video. Each keyframe is associated with a timestamp\\n{time1, time2, . . . ,timen} that correspond to significant visual\\ncontent changes in the video.\\nVideo Xi is segmented into several defined time\\nintervals{G1, G2, . . . ,Gx }, where each group Gi represents\\npart of the transcript and the associated keyframes cor-\\nresponding to the time bounds of that group. For each\\ngroup Gi, the transcript and keyframes {pi1, pi2, . . . ,pij} and\\n{ki1, ki2, . . . ,kik }, respectively, were analyzed.\\nPrediction: The evolution of taxonomies across the seg-\\nmented groups within the video is represented by\\n{G1 : [µ11T11, µ12T12, . . . , µ1pT1p],\\nVOLUME 13, 2025 30609'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 13, 'page_label': '30610'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\nG2 : [µ21T21, µ22T22, . . . , µ2pT2p],\\n. . . ,\\nGx : [µx1Tx1, µx2Tx2, . . . , µxpTxp]} (11)\\nwhere:\\n• Gi represents each segment of the video.\\n• Tij is a specific taxonomy identified within group Gi.\\n• µij is the multiplier indicating the frequency and\\nrelevance of taxonomy Tij within that specific group.\\nThus, the variation of certain taxonomies over a video\\nwere mapped to view the thematic progression dynami-\\ncally. To illustrate the extension of the framework, it is\\napplied to the YouTube ‘‘https://www.youtube.com/watch?v=\\neIcnvKLdxLU’’ to identify topics evolving in it. The output\\ngenerated by the taxonomy over time is depicted in Figure 6.\\nAlthough these results showed Figure 6 that dynamic\\ntaxonomy retrieval was feasible across videos, some areas\\nhad more room for improvement. Further video segmentation\\ntechniques should be explored in finding logical boundaries\\nwithout using a fixed time interval, as performed in this fea-\\nsibility study, for more effective execution. These will incor-\\nporate topic transitions and topic-aware sentiment analysis to\\nattain accuracy and relevance for the Taxonomies. Moreover,\\nfuture research may test these finalized methodologies on\\nlong-range video datasets, because a dataset similar to\\nYouTube-8M is incomplete in capturing complexities for a\\nlong-range video dataset.\\nVI. CONCLUSION\\nThis study proposed a novel framework for the multimodal\\nretrieval approach to industrially compliant contextual adver-\\ntising through the use of state-of-the-art NLP and multimodal\\nanalysis techniques. Transformer-based models, particularly\\nthe BERTopic and language models, enable the achievement\\nof video representations aligned with content taxonomies for\\ntargeted and relevant advertisements in a privacy-compliant\\nmanner. Our methodology will be particularly effective for\\nnoisy, multilingual, user-generated content and offer a highly\\nscalable solution for the advertising industry. It will be\\nan increasingly relevant solution as the demand for A V oD\\nservices continues to grow, and with traditional behavior-\\nbased targeting becoming less effective because of changing\\nprivacy regulations. This research contributes to the field\\nof digital marketing by an advancement in techniques of\\nprogrammatic advertising and hence provides a scalable\\nmeans of improving ad relevancy, user engagement, and total\\neffectiveness on video platforms.\\nBuilding on this research, enhancements in the dynamic\\ntracking of taxonomy changes over time would be a good\\navenue for future work. Such a development would address\\none of the most important demands in programmatic adver-\\ntising: precise ad alignment in the evolution of video content.\\nTo this end, refining video segmentation techniques to capture\\nlogical content boundaries dynamically is a promising\\ndirection. This can be taken further by adding topic-aware\\nsentiment analysis to provide even richer contextual insight\\ninto the thematic and emotional subtleties of video content.\\nThese efforts go toward making contextual advertising\\nscalable, accurate, and flexible in driving superior user\\nengagement and overall efficiency across video platforms.\\nACKNOWLEDGMENT\\nThe authors would like to thank the editors and anonymous\\nreviewers from IEEE ACCESS for their constructive and insight-\\nful comments and suggestions which helped in substantially\\nimproving the presentation of this article.\\nREFERENCES\\n[1] C. Grece, ‘‘Trends in the VOD market in EU28,’’ Eur. Audiovisual\\nObservatory, Strasbourg, France, Tech. Rep., 2021.\\n[2] S. Broughton Micova and S. Jacques, ‘‘Platform power in the video\\nadvertising ecosystem,’’ Internet Policy Rev., vol. 9, no. 4, pp. 1–28, 2020.\\n[3] Z. Liu, U. Iqbal, and N. Saxena, ‘‘Opted out, yet tracked: Are regulations\\nenough to protect your privacy?’’ 2022, arXiv:2202.00885.\\n[4] M. Veale and F. Zuiderveen Borgesius, ‘‘Adtech and real-time bidding\\nunder European data protection law,’’ German Law J., vol. 23, no. 2,\\npp. 226–256, Mar. 2022.\\n[5] T. Mei, J. Guo, X.-S. Hua, and F. Liu, ‘‘AdOn: Toward contextual overlay\\nin-video advertising,’’ Multimedia Syst., vol. 16, nos. 4–5, pp. 335–344,\\nAug. 2010.\\n[6] T. Mei and X.-S. Hua, ‘‘Contextual Internet multimedia advertising,’’ Proc.\\nIEEE, vol. 98, no. 8, pp. 1416–1433, Aug. 2010.\\n[7] T. Kozlova, ‘‘Efficiency of business and intercultural communication:\\nMultilingual advertising discourse,’’ in Proc. III Int. Sci. Congr. Soc.\\nAmbient Intell. (ISC-SAI). Amsterdam, The Netherlands: Atlantis Press,\\n2020, pp. 272–278.\\n[8] T. Wang and R. L. Bailey, ‘‘Processing peripherally placed advertising:\\nThe effect of thematic ad-content congruence and arousing content on\\nthe effectiveness of in-video overlay advertising,’’ J. Interact. Advertising,\\nvol. 23, no. 3, pp. 203–220, Jul. 2023.\\n[9] S. Segev, W. Wang, and J. Fernandes, ‘‘The effects of ad–context\\ncongruency on responses to advertising in blogs: Exploring the role of issue\\ninvolvement,’’Int. J. Advertising, vol. 33, no. 1, pp. 17–36, Jan. 2014.\\n[10] M. Reisenbichler and T. Reutterer, ‘‘Topic modeling in marketing: Recent\\nadvances and research opportunities,’’ J. Bus. Econ., vol. 89, no. 3,\\npp. 327–356, Apr. 2019.\\n[11] N. Klym and D. Clark, ‘‘The future of the ad-supported Internet\\necosystem,’’ Tech. Rep., 2019.\\n[12] J. K. Chalaby, ‘‘The streaming industry and the platform economy: An\\nanalysis,’’Media, Culture Soc., vol. 46, no. 3, pp. 552–571, Apr. 2024.\\n[13] Z. Sherman, Modern Advertising and the Market for Audience Attention:\\nThe U.S. Advertising Industry’s Turn-of-the-Twentieth-Century Transition.\\nEvanston, IL, USA: Routledge, 2019.\\n[14] A. Turillazzi, M. Taddeo, L. Floridi, and F. Casolari, ‘‘The digital services\\nact: An analysis of its ethical, legal, and social implications,’’ Law, Innov.\\nTechnol., vol. 15, no. 1, pp. 83–106, Jan. 2023.\\n[15] T. Mei, L. Yang, X.-S. Hua, H. Wei, and S. Li, ‘‘VideoSense: A contextual\\nvideo advertising system,’’ in Proc. 15th ACM Int. Conf. Multimedia,\\nSep. 2007, pp. 463–464.\\n[16] K. Okada, E. S. de Moura, M. Cristo, D. Fernandes, M. A. Gonçalves,\\nand K. Berlt, ‘‘Advertisement selection for online videos,’’ in Proc. 18th\\nBrazilian Symp. Multimedia Web, Oct. 2012, pp. 367–374.\\n[17] C. Xiang, T. V . Nguyen, and M. Kankanhalli, ‘‘SalAd: A multimodal\\napproach for contextual video advertising,’’ in Proc. IEEE Int. Symp.\\nMultimedia (ISM), Dec. 2015, pp. 211–216.\\n[18] H. Zhang, X. Cao, J. K. L. Ho, and T. W. S. Chow, ‘‘Object-level video\\nadvertising: An optimization framework,’’ IEEE Trans. Ind. Informat.,\\nvol. 13, no. 2, pp. 520–531, Apr. 2017.\\n[19] G. Wang, L. Zhuo, J. Li, D. Ren, and J. Zhang, ‘‘An efficient method\\nof content-targeted online video advertising,’’ J. Vis. Commun. Image\\nRepresent., vol. 50, pp. 40–48, Jan. 2018.\\n[20] E. Häglund and J. Björklund, ‘‘AI-driven contextual advertising: Toward\\nrelevant messaging without personal data,’’ J. Current Issues Res.\\nAdvertising, vol. 45, no. 3, pp. 301–319, Jul. 2024.\\n30610 VOLUME 13, 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 14, 'page_label': '30611'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\n[21] R. Tapu, B. Mocanu, and T. Zaharia, ‘‘DEEP-AD: A multimodal temporal\\nvideo segmentation framework for online video advertising,’’ IEEE Access,\\nvol. 8, pp. 99582–99597, 2020.\\n[22] B. Mocanu and R. Tapu, ‘‘SemanticAd: A multimodal contextual\\nadvertisement framework for online video streaming platforms,’’ IEEE\\nAccess, vol. 12, pp. 63142–63155, 2024.\\n[23] X. Song, B. Xu, and Y .-G. Jiang, ‘‘Predicting content similarity via\\nmultimodal modeling for video-in-video advertising,’’ IEEE Trans.\\nCircuits Syst. Video Technol., vol. 31, no. 2, pp. 569–581, Feb. 2021.\\n[24] W. Wu, Y . Zhao, Y . Xu, X. Tan, D. He, Z. Zou, J. Ye, Y . Li, M. Yao, Z. Dong,\\nand Y . Shi, ‘‘DSANet: Dynamic segment aggregation network for video-\\nlevel representation learning,’’ in Proc. 29th ACM Int. Conf. Multimedia,\\nOct. 2021, pp. 1903–1911.\\n[25] S. Jadon and M. Jasim, ‘‘Unsupervised video summarization framework\\nusing keyframe extraction and video skimming,’’ in Proc. IEEE 5th Int.\\nConf. Comput. Commun. Autom. (ICCCA), Oct. 2020, pp. 140–145.\\n[26] V . Truong, ‘‘Natural language processing in advertising—A systematic\\nliterature review,’’ in Proc. 5th Asia Conf. Mach. Learn. Comput.\\n(ACMLC), Dec. 2022, pp. 89–98.\\n[27] H. Zhang, Z. Ding, M. Sharid Kayes Dipu, P. Lv, Y . Huang, H. Suleiman\\nAbdullahi, A. Zhang, Z. Song, and Y . Wang, ‘‘Identification of illegal\\noutdoor advertisements based on CLIP fine-tuning and OCR technology,’’\\nIEEE Access, vol. 12, pp. 92976–92987, 2024.\\n[28] Q. Yang, M. Ongpin, S. Nikolenko, A. Huang, and A. Farseev, ‘‘Against\\nopacity: Explainable AI and large language models for effective digital\\nadvertising,’’ in Proc. 31st ACM Int. Conf. Multimedia , Oct. 2023,\\npp. 9299–9305.\\n[29] H. Jelodar, Y . Wang, C. Yuan, X. Feng, X. Jiang, Y . Li, and L. Zhao, ‘‘Latent\\nDirichlet allocation (LDA) and topic modeling: Models, applications,\\na survey,’’ Multimedia Tools Appl. , vol. 78, no. 11, pp. 15169–15211,\\nJun. 2019.\\n[30] J. Yu, Z. Qin, T. Wan, and X. Zhang, ‘‘Feature integration analysis of\\nbag-of-features model for image retrieval,’’ Neurocomputing, vol. 120,\\npp. 355–364, Nov. 2013.\\n[31] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efficient estimation of\\nword representations in vector space,’’ 2013, arXiv:1301.3781.\\n[32] R. Pal, A. A. Sekh, D. P. Dogra, S. Kar, P. P. Roy, and D. K. Prasad, ‘‘Topic-\\nbased video analysis: A survey,’’ ACM Comput. Surveys, vol. 54, no. 6,\\npp. 1–34, Jul. 2022.\\n[33] M. Grootendorst, ‘‘BERTopic: Neural topic modeling with a class-based\\nTF-IDF procedure,’’ 2022, arXiv:2203.05794.\\n[34] N. Prakash, H. Wang, N. K. Hoang, M. S. Hee, and R. K.-W. Lee,\\n‘‘PromptMTopic: Unsupervised multimodal topic modeling of memes\\nusing large language models,’’ in Proc. 31st ACM Int. Conf. Multimedia,\\nOct. 2023, pp. 621–631.\\n[35] C. Févotte and J. Idier, ‘‘Algorithms for nonnegative matrix factorization\\nwith the β-divergence,’’Neural Comput., vol. 23, no. 9, pp. 2421–2456,\\nSep. 2011.\\n[36] F. Bianchi, S. Terragni, D. Hovy, D. Nozza, and E. Fersini, ‘‘Cross-\\nlingual contextualized topic models with zero-shot learning,’’ 2020,\\narXiv:2004.07737.\\n[37] Inf. Technol. Lab. (2021). Content Taxonomy 3.0. Accessed: Jul. 19, 2024.\\n[Online]. Available: https://iabtechlab.com/standards/content-taxonomy/\\n[38] L. McInnes, J. Healy, and J. Melville, ‘‘UMAP: Uniform manifold approx-\\nimation and projection for dimension reduction,’’ 2018, arXiv:1802.03426.\\n[39] L. McInnes, J. Healy, and S. Astels, ‘‘Hdbscan: Hierarchical density based\\nclustering,’’J. Open Source Softw., vol. 2, no. 11, p. 205, Mar. 2017.\\n[40] J. Ramos, ‘‘Using TF-IDF to determine word relevance in document\\nqueries,’’ in Proc. 1st Instructional Conf. Mach. Learn., vol. 242, no. 1,\\n2003, pp. 29–48.\\n[41] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,\\n‘‘Learning transferable visual models from natural language supervision,’’\\nin Proc. Int. Conf. Mach. Learn., Jan. 2021, pp. 8748–8763.\\n[42] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words:\\nTransformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\\n[43] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\\nJun. 2016, pp. 770–778.\\n[44] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\\nlarge-scale image recognition,’’ 2014, arXiv:1409.1556.\\n[45] S. Cai, L. Qiu, X. Chen, Q. Zhang, and L. Chen, ‘‘Semantic-enhanced\\nimage clustering,’’ in Proc. AAAI Conf. Artif. Intell., Jun. 2023, vol. 37,\\nno. 6, pp. 6869–6878.\\n[46] Z. Dang, C. Deng, X. Yang, K. Wei, and H. Huang, ‘‘Nearest neighbor\\nmatching for deep clustering,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\\nPattern Recognit. (CVPR), Jun. 2021, pp. 13688–13697.\\n[47] W. V . Gansbeke, S. Vandenhende, S. Georgoulis, M. Proesmans, and\\nL. V . Gool, ‘‘SCAN: Learning to classify images without labels,’’ in\\nProc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, May 2020,\\npp. 268–285.\\n[48] X. Dong, J. Bao, T. Zhang, D. Chen, S. Gu, W. Zhang, L. Yuan, D. Chen,\\nF. Wen, and N. Yu, ‘‘CLIP itself is a strong fine-tuner: Achieving 85.7%\\nand 88.0% Top-1 accuracy with ViT-B and ViT-L on ImageNet,’’ 2022,\\narXiv:2212.06138.\\n[49] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\\n‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog ,\\nvol. 1, no. 8, p. 9, Feb. 2019.\\n[50] H. Face. (2023). Vit-gpt2 Image Captioning. [Online]. Available:\\nhttps://huggingface.co/nlpconnect/vit-gpt2-image-captioning\\n[51] F. Feng, Y . Yang, D. Cer, N. Arivazhagan, and W. Wang, ‘‘Language-\\nagnostic BERT sentence embedding,’’ 2020, arXiv:2007.01852.\\n[52] N. Reimers and I. Gurevych, ‘‘Sentence-BERT: Sentence embeddings\\nusing Siamese BERT-networks,’’ 2019, arXiv:1908.10084.\\n[53] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\\nof deep bidirectional transformers for language understanding,’’ 2018,\\narXiv:1810.04805.\\n[54] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\\npretraining approach,’’ 2019, arXiv:1907.11692.\\n[55] J. Libovický, R. Rosa, and A. Fraser, ‘‘How language-neutral is multilin-\\ngual BERT?’’ 2019, arXiv:1911.03310.\\n[56] FFmpeg Developers. (2024). Ffmpeg. [Online]. Available: https://ffmpeg.\\norg/\\n[57] OpenAI. (2024). Whisper. Accessed: Jul. 19, 2024. [Online]. Available:\\nhttps://github.com/openai/whisper\\n[58] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan,\\nand S. Vijayanarasimhan, ‘‘YouTube-8M: A large-scale video classifica-\\ntion benchmark,’’ 2016, arXiv:1609.08675.\\n[59] R. B. Pereira, A. Plastino, B. Zadrozny, and L. H. C. Merschmann, ‘‘Cor-\\nrelation analysis of performance measures for multi-label classification,’’\\nInf. Process. Manage., vol. 54, no. 3, pp. 359–369, May 2018.\\n[60] M. Sokolova and G. Lapalme, ‘‘A systematic analysis of performance\\nmeasures for classification tasks,’’ Inf. Process. Manage., vol. 45, no. 4,\\npp. 427–437, Jul. 2009.\\n[61] M. S. Sorower, ‘‘A literature survey on algorithms for multi-label\\nlearning,’’Oregon State Univ., Corvallis, vol. 18, no. 1, p. 25, 2010.\\n[62] N. Ghamrawi and A. McCallum, ‘‘Collective multi-label classification,’’ in\\nProc. 14th ACM Int. Conf. Inf. Knowl. Manage., Oct. 2005, pp. 195–200.\\n[63] Z. Chase Lipton, C. Elkan, and B. Narayanaswamy, ‘‘Thresholding\\nclassifiers to maximize F1 score,’’ 2014, arXiv:1402.1892.\\n[64] J. Chang, S. Gerrish, C. Wang, J. Boyd-Graber, and D. M. Blei, ‘‘Reading\\ntea leaves: How humans interpret topic models,’’ in Proc. Adv. Neural Inf.\\nProcess. Syst., vol. 22, Dec. 2009, pp. 288–296.\\n[65] H. Won Chung et al., ‘‘Scaling instruction-finetuned language models,’’\\n2022, arXiv:2210.11416.\\n[66] R. Anil et al., ‘‘PaLM 2 technical report,’’ 2023, arXiv:2305.10403.\\n[67] F. Viegas, A. Pereira, W. Cunha, C. França, C. Andrade, E. Tuler, L. Rocha,\\nand M. A. Gonçalves, ‘‘Exploiting contextual embeddings in hierarchical\\ntopic modeling and investigating the limits of the current evaluation\\nmetrics,’’ in Computational Linguistics, 2024, pp. 1–59.\\n[68] N. Gerasimenko, A. Chernyavskiy, M. Nikiforova, A. Ianina, and\\nK. V orontsov, ‘‘Incremental topic modeling for scientific trend topics\\nextraction,’’ in Proc. Int. Conf. Dialogue, Jun. 2023, pp. 1–5.\\n[69] Interact. Advertising Bur. (2024). Content Taxonomy 3.1. [Online].\\nAvailable: https://iabtechlab.com/standards/content-taxonomy/\\n[70] T. Agrawal and T. Agrawal, ‘‘Solving time and memory constraints,’’ in\\nHyperparameter Optimization in Machine Learning: Make Your Machine\\nLearning and Deep Learning Models More Efficient, 2021, pp. 53–80.\\n[71] X. Sun, P. Zhang, P. Zhang, H. Shah, K. Saenko, and X. Xia, ‘‘DIME-\\nFM: DIstilling multimodal and efficient foundation models,’’ in Proc.\\nIEEE/CVF Int. Conf. Comput. Vis., Jan. 2023, pp. 15521–15533.\\nVOLUME 13, 2025 30611'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-18T14:00:40+05:30', 'moddate': '2025-02-19T10:27:27-05:00', 'ieee article id': '10890950', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542562', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Explainable Video Topics for Content Taxonomy: A Multimodal Retrieval Approach to Industry-Compliant Contextual Advertising', 'source': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'total_pages': 16, 'page': 15, 'page_label': '30612'}, page_content='W. De Silva, A. Fernando: Explainable Video Topics for Content Taxonomy\\n[72] H. F. Safetensors. Safetensors: A Simple and Safe File Format for Neural\\nNetwork Weights. [Online]. Available: https://github.com/huggingface/\\nsafetensors\\n[73] B. Casey, J. C. S. Santos, and M. Mirakhorli, ‘‘A large-scale exploit\\ninstrumentation study of AI/ML supply chain attacks in hugging face\\nmodels,’’ 2024, arXiv:2410.04490.\\n[74] O. K. Chong, H.-N. Goh, and J. See, ‘‘What modality matters? Exploiting\\nhighly relevant features for video advertisement insertion,’’ in Proc. IEEE\\nInt. Conf. Image Process. (ICIP), Oct. 2023, pp. 3344–3348.\\n[75] M. Singh and R. Lamba, ‘‘Proposing contextually relevant advertisements\\nfor online videos,’’ in Proc. 1st Symp. Mach. Learn. Metaheuristics\\nAlgorithms, Appl., Trivandrum, India. Cham, Switzerland: Springer,\\nJan. 2020, pp. 218–224.\\n[76] D. Davtyan and A. Tashchian, ‘‘Thematic congruency in the context of\\nbrand placements: Tests of memory and attitude measures,’’ J. Current\\nIssues Res. Advertising, vol. 43, no. 3, pp. 319–335, Jul. 2022.\\n[77] M. Dahlén, S. Rosengren, F. Törn, and N. Öhman, ‘‘Could placing\\nADS wrong be right?: Advertising effects of thematic incongruence,’’ J.\\nAdvertising, vol. 37, no. 3, pp. 57–67, Sep. 2008.\\n[78] C. Zhang, Z. Yang, X. He, and L. Deng, ‘‘Multimodal intelligence:\\nRepresentation learning, information fusion, and applications,’’ IEEE J.\\nSel. Topics Signal Process., vol. 14, no. 3, pp. 478–493, Mar. 2020.\\n[79] K. Gadzicki, R. Khamsehashari, and C. Zetzsche, ‘‘Early vs late fusion in\\nmultimodal convolutional neural networks,’’ in Proc. IEEE 23rd Int. Conf.\\nInf. Fusion, Jul. 2020, pp. 1–6.\\n[80] B. Zhao, C. Cheng, Z. Peng, X. Dong, and G. Meng, ‘‘Detecting the early\\ndamages in structures with nonlinear output frequency response functions\\nand the CNN-LSTM model,’’ IEEE Trans. Instrum. Meas., vol. 69, no. 12,\\npp. 9557–9567, Dec. 2020.\\n[81] M. Qiao, S. Yan, X. Tang, and C. Xu, ‘‘Deep convolutional and LSTM\\nrecurrent neural networks for rolling bearing fault diagnosis under strong\\nnoises and variable loads,’’ IEEE Access, vol. 8, pp. 66257–66269, 2020.\\n[82] L. Shang, Z. Zhang, F. Tang, Q. Cao, H. Pan, and Z. Lin, ‘‘CNN-LSTM\\nhybrid model to promote signal processing of ultrasonic guided Lamb\\nwaves for damage detection in metallic pipelines,’’ Sensors, vol. 23, no. 16,\\np. 7059, Aug. 2023.\\n[83] J. Wu, Y . Liang, F. Han, H. Akbari, Z. Wang, and C. Yu, ‘‘Scaling\\nmultimodal pre-training via cross-modality gradient harmonization,’’ in\\nProc. Adv. Neural Inf. Process. Syst., Jan. 2022, pp. 36161–36173.\\n[84] T. Saeki, S. Maiti, X. Li, S. Watanabe, S. Takamichi, and H. Saruwatari,\\n‘‘Learning to speak from text: Zero-shot multilingual text-to-Speech with\\nunsupervised text pretraining,’’ 2023, arXiv:2301.12596.\\n[85] J. Pfau, A. T. Young, J. Wei, M. L. Wei, and M. J. Keiser, ‘‘Robust\\nsemantic interpretability: Revisiting concept activation vectors,’’ 2021,\\narXiv:2104.02768.\\n[86] K. A. Nguyen, S. S. I. Walde, and N. T. Vu, ‘‘Distinguishing antonyms and\\nsynonyms in a pattern-based neural network,’’ 2017, arXiv:1701.02962.\\n[87] D. Kozlowski, C. Pradier, and P. Benz, ‘‘Generative AI for automatic topic\\nlabelling,’’ 2024, arXiv:2408.07003.\\n[88] E. Zosa and L. Pivovarova, ‘‘Multilingual and multimodal topic modelling\\nwith pretrained embeddings,’’ 2022, arXiv:2211.08057.\\n[89] L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Al-Dujaili, Y . Duan, O. Al-\\nShamma, J. Santamaría, M. A. Fadhel, M. Al-Amidie, and L. Farhan,\\n‘‘Review of deep learning: Concepts, CNN architectures, challenges,\\napplications, future directions,’’ J. Big Data, vol. 8, no. 1, pp. 1–74,\\nMar. 2021.\\n[90] A. Rahimi, S. Datta, D. Kleyko, E. P. Frady, B. Olshausen, P. Kanerva, and\\nJ. M. Rabaey, ‘‘High-dimensional computing as a nanoscalable paradigm,’’\\nIEEE Trans. Circuits Syst. I, Reg. Papers, vol. 64, no. 9, pp. 2508–2521,\\nSep. 2017.\\nWARUNA DE SILVA(Graduate Student Member,\\nIEEE) received the B.Sc. and M.B.A. degrees\\nin civil engineering from the University of\\nMoratuwa, Sri Lanka, in 2000 and 2010, respec-\\ntively. He is currently pursuing the Ph.D. degree\\nin computer and information sciences with the\\nUniversity of Strathclyde, U.K. With over 20 years\\nof experience in the global IT industry, he has\\nexcelled in software services, product develop-\\nment, and successful project delivery for enterprise\\nU.S. and U.K. clients. His research interests include natural language\\nprocessing, machine learning, multimedia data mining, and programmatic\\nadvertising.\\nANIL FERNANDO (Senior Member, IEEE)\\nreceived the B.Sc. degree (Hons.) in electron-\\nics and telecommunication engineering from the\\nUniversity of Moratuwa, Sri Lanka, in 1995, the\\nM.Sc. degree (Hons.) in communications from\\nAsian Institute of Technology, Bangkok, Thailand,\\nin 1997, and the Ph.D. degree in computer science\\n(video coding and communications) from the Uni-\\nversity of Bristol, U.K., in 2001. He is currently\\na Professor in video coding and communications\\nwith the Department of Computer and Information Sciences, University of\\nStrathclyde, U.K., where he leads the Video Coding and Communication\\nResearch Team and also a Visiting Professor with the Center for Vision,\\nSpeech and Signal Processing (CVSSP), University of Surrey, U.K.\\nHe has been working with all major EU broadcasters, BBC, and major\\nEuropean media companies/SMEs in the last decade to provide innovative\\nmedia technologies for British and EU citizens. He has graduated more\\nthan 110 Ph.D. students and is currently supervising 20 Ph.D. students.\\nHe has worked on major national and international multidisciplinary research\\nprojects and led most of them. He has published more than 430 papers in\\ninternational journals and conference proceedings and has published a book\\non 3D video broadcasting. His main research interests include video coding\\nand communications, machine learning, artificial intelligence, semantic\\ncommunications, signal processing, networking and communications, inter-\\nactive systems, resource optimization in 6G, distributed technologies, media\\nbroadcasting, and quality of experience.\\n30612 VOLUME 13, 2025')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_documents(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a27fa91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(docs, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split raw documents into smaller chunks for embedding & retrieval.\n",
    "\n",
    "    Args:\n",
    "        docs: list of Documents (from ingestion).\n",
    "        chunk_size: target characters per chunk.\n",
    "        chunk_overlap: overlap between chunks to preserve context.\n",
    "\n",
    "    Returns:\n",
    "        List of chunked Documents.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26a1d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_retriever(chunks, k: int = 4):\n",
    "#     # requires OPENAI_API_KEY in your env\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "#     vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "#     return vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "def build_hybrid_retriever(chunks, k: int = 6, alpha: float = 0.6):\n",
    "    \"\"\"\n",
    "    Hybrid = BM25 (keywords) + FAISS (dense). \n",
    "    alpha = weight for dense; (1 - alpha) for BM25.\n",
    "    \"\"\"\n",
    "    # Keyword retriever (great for 'DOI', dates, author names)\n",
    "    bm25 = BM25Retriever.from_documents(chunks)\n",
    "    bm25.k = k\n",
    "\n",
    "    # Dense retriever (semantic)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "    dense = vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # Fuse with reciprocal-rank style\n",
    "    return EnsembleRetriever(retrievers=[bm25, dense],\n",
    "                             weights=[1.0 - alpha, alpha])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qa_chain(retriever, model: str = \"gpt-4o-mini\"):\n",
    "    llm = ChatOpenAI(model=model, temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Use ONLY the context to answer. If not found, say you don't know.\\n\\n\"\n",
    "        \"Question: {input}\\n\\nContext:\\n{context}\"\n",
    "    )\n",
    "    doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    return create_retrieval_chain(retriever, doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73f807b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask(question: str, chunks):\n",
    "#     retriever = build_retriever(chunks, k=4)\n",
    "#     qa = make_qa_chain(retriever)\n",
    "#     resp = qa.invoke({\"input\": question})\n",
    "\n",
    "#     # pretty sources (filename + 1-based page if available)\n",
    "#     sources = []\n",
    "#     for d in resp.get(\"context\", []):\n",
    "#         meta = d.metadata or {}\n",
    "#         page = meta.get(\"page\")\n",
    "#         sources.append({\n",
    "#             \"file\": meta.get(\"filename\") or meta.get(\"source\"),\n",
    "#             \"page\": (page + 1) if isinstance(page, int) else page\n",
    "#         })\n",
    "#     return resp.get(\"answer\"), sources\n",
    "\n",
    "\n",
    "def ask(question: str, retriever):\n",
    "    qa = make_qa_chain(retriever)\n",
    "    resp = qa.invoke({\"input\": question})\n",
    "\n",
    "    sources = []\n",
    "    for d in resp.get(\"context\", []):\n",
    "        meta = d.metadata or {}\n",
    "        page = meta.get(\"page\")\n",
    "        sources.append({\n",
    "            \"file\": meta.get(\"filename\") or meta.get(\"source\"),\n",
    "            \"page\": (page + 1) if isinstance(page, int) else page\n",
    "        })\n",
    "    return resp.get(\"answer\"), sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a1a9500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The author's email address is anil.fernando@strath.ac.uk.\n",
      "Sources: [{'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 14}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 10}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 6}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 2}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 9}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 9}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 1}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 16}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 16}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 15}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 1}]\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # ensure OPENAI_API_KEY is in env\n",
    "\n",
    "docs   = ingest_documents(path)\n",
    "chunks = split_documents(docs, chunk_size=1200, chunk_overlap=200)\n",
    "\n",
    "# Use HYBRID for exact fields like DOI\n",
    "retriever = build_hybrid_retriever(chunks, k=6, alpha=0.4)  # slightly BM25-heavy\n",
    "\n",
    "answer, sources = ask(\"what is authors email adress?\", retriever)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Sources:\", sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know.\n",
      "Sources: [{'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 10}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 6}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 2}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 9}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 9}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 3}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 1}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 16}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 16}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 15}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 14}, {'file': 'D:\\\\LANGCHAIN\\\\Projects\\\\PDF_QA\\\\Explainable_Video_Topics_for_Content_Taxonomy_A_Multimodal_Retrieval_Approach_to_Industry-Compliant_Contextual_Advertising 1 (1).pdf', 'page': 1}]\n"
     ]
    }
   ],
   "source": [
    "answer, sources = ask(\"what is authers email adress?\", retriever)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Sources:\", sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9c38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

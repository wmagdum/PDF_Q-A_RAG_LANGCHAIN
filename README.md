# ğŸ“„ PDF Q&A â€” Hybrid RAG (Streamlit + LangChain + FAISS)

An interactive, production-grade **PDF questionâ€“answering** app built with **Streamlit**, **LangChain**, and a **hybrid retriever** (BM25 + FAISS).  
Upload one or multiple PDFs, control chunking, temperature, and retrieval behavior, and get **grounded answers with sources**.  
Indexes are **persisted on disk** so you donâ€™t re-embed your corpus every run.

---
## ğŸ–¼ï¸ Demo Screenshot

![PDF Q&A Demo](assets/Demo.jpg)



## âœ¨ Features

- **Multi-PDF upload** (drag & drop)
- **Modular pipeline**: ingest â†’ split â†’ embed â†’ vector store â†’ hybrid retrieval â†’ answer
- **Hybrid search**: BM25 (keywords) + FAISS (dense vectors) via reciprocal-rank fusion
- **Full user controls**:
  - OpenAI API key (entered at runtime)
  - Chunk size & overlap
  - Topâ€‘K per retriever
  - Dense weight **Î±** (0 â†’ keyword-only, 1 â†’ dense-only)
  - LLM model & temperature
  - Embedding model (OpenAI)
- **Persistent FAISS index** on disk (no re-embedding on subsequent runs)
- **Source previews**: expand to view retrieved chunks and the exact pages used
- **Debug options**: show full retrieved context and raw chain response

---

## ğŸ§± Architecture

```
Uploads â†’ Ingest (PyPDFLoader) â†’ Split (RecursiveCharacterTextSplitter)
â†’ Embeddings (OpenAI) â†’ FAISS index (persisted to disk)
â†’ Hybrid Retriever (BM25 + FAISS) â†’ LLM (ChatOpenAI) â†’ Answer + Sources
```

- Indexes are saved under `indexes/<corpus_sha1_prefix>/` with a guard `meta.json` (embed model + split params).
- On subsequent runs with the **same corpus + params**, the app **loads** the FAISS index instead of rebuilding.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py               # Streamlit UI (controls, upload, build index, ask)
â”œâ”€â”€ rag_utils.py         # RAG helpers (ingest, split, persist FAISS, hybrid retriever, QA chain)
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # This file
```

---

## ğŸ”§ Requirements

- Python **3.10+** recommended
- An OpenAI API key with access to the selected chat & embedding models

Install deps:

```bash
pip install -r requirements.txt
```

> On Apple Silicon/Windows, `faiss-cpu` from `pip` works for most cases. If you hit FAISS install issues, check the FAISS wheel for your platform or consider using a Conda environment.

---

## ğŸš€ Run Locally

**Windows / macOS / Linux** (inside your virtual environment):

```bash
streamlit run app.py
# or if streamlit isn't on PATH:
python -m streamlit run app.py
```

Then open the URL Streamlit prints (e.g., `http://localhost:8501`).

### First Use (Interactive Flow)

1. **Enter your OpenAI API key** in the sidebar.
2. **Upload** one or more PDF files.
3. Set **chunk size** / **overlap**, retrieval **Topâ€‘K**, and **Î±** (dense weight).
4. Click **â€œBuild / Load Indexâ€** to split, embed, and persist the FAISS index.
5. Ask questions in the input box and review answers & **sources**.

> The index is reused when the **corpus and parameters** (embed model, chunk size, overlap) match. Changing them will trigger a rebuild.

---

## âš™ï¸ Configuration Details

### Chunking
- Sensible defaults: `chunk_size=1200`, `chunk_overlap=200`
- For front-matter facts (DOI, author email), smaller chunks can help: e.g. `600 / 100`

### Hybrid Retrieval
- **Î±** controls BM25 vs dense weighting:
  - Î± = **0.4** â†’ keywordâ€‘leaning (good for exact strings like â€œDOIâ€)
  - Î± = **0.7** â†’ semanticâ€‘leaning (good for conceptual questions)

### Embeddings & Models
- Embeddings: `text-embedding-3-small` (fast & affordable) or `-large` (higher quality)
- Chat models: `gpt-4o-mini` (default), `gpt-4o`, etc.
- Temperature: typically **0.0** for QA; increase for more creative synthesis.

### Index Persistence
- Index path: `indexes/<corpus_sha1_prefix>/`
- Guarded by `meta.json` (embed model + chunking params).  
- To force rebuild, delete the folder or change params.

---

## ğŸ§ª Example Commands

```bash
# Install & run
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
streamlit run app.py

# Choose port
streamlit run app.py --server.port 8502
```
